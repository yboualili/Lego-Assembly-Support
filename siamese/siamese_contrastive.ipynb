{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self,imageFolderDataset,false_dataset, transform=None):\n",
    "        self.imageFolderDataset = imageFolderDataset\n",
    "        self.false_dataset = false_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset)\n",
    "        #We need to approximately 50% of images to be positive\n",
    "        should_get_same_class = random.randint(0,1)\n",
    "        if should_get_same_class:\n",
    "            target = 1\n",
    "            while True:\n",
    "                #Look untill the same stage image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset)\n",
    "                if img0_tuple[1] == img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            target = 0\n",
    "            include_false =random.randint(0,4)\n",
    "\n",
    "            # in 3 out of 4 cases, choose a false image from the same stage\n",
    "            # in the 1 out of 4 cases choose a false image from another stage\n",
    "            if include_false < 3:\n",
    "                while True:\n",
    "                    img1_tuple = random.choice(self.false_dataset)\n",
    "                    if img0_tuple[1] == img1_tuple[1]:\n",
    "                        break\n",
    "            else:\n",
    "                img1_tuple = random.choice(self.false_dataset)\n",
    "\n",
    "\n",
    "        img0 = img0_tuple[0].convert('RGB')\n",
    "        width, height = img0.size\n",
    "        if width > height:\n",
    "            img0 = img0.transpose(Image.TRANSPOSE)\n",
    "        img1 = img1_tuple[0].convert('RGB')\n",
    "        width, height = img1.size\n",
    "        if width > height:\n",
    "            img1 = img1.transpose(Image.TRANSPOSE)\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "\n",
    "        return img0, img1, torch.from_numpy(np.array([1-target], dtype=np.float32))\n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset) + len(self.false_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "folder_dataset = datasets.ImageFolder(root=r\"D:\\PycharmProjects\\AISS\\siamese\\dataset_true\")\n",
    "false_dataset = datasets.ImageFolder(root=r\"D:\\PycharmProjects\\AISS\\siamese\\dataset_false\")\n",
    "\n",
    "folder_dataset_test = datasets.ImageFolder(root=r\"D:\\PycharmProjects\\AISS\\siamese\\dataset_true_val\")\n",
    "false_dataset_test = datasets.ImageFolder(root=r\"D:\\PycharmProjects\\AISS\\siamese\\dataset_false_val\")\n",
    "\n",
    "# Augment + Resize the images and transform to tensors\n",
    "transformation = transforms.Compose([\n",
    "                                    transforms.Resize((100, 100)),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomVerticalFlip(),\n",
    "                                     transforms.ToTensor()\n",
    "\n",
    "                                    ])\n",
    "\n",
    "transformation_test = transforms.Compose([\n",
    "                                    transforms.Resize((100, 100)),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomVerticalFlip(),\n",
    "                                     transforms.ToTensor()\n",
    "                                    ])\n",
    "\n",
    "# Initialize the dataset\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,false_dataset=false_dataset,\n",
    "                                        transform=transformation)\n",
    "\n",
    "siamese_dataset_test = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,false_dataset=false_dataset_test,\n",
    "                                        transform=transformation_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# create the dataloader\n",
    "trainloader = DataLoader(siamese_dataset,\n",
    "                        batch_size=16, drop_last=True)\n",
    "\n",
    "testloader = DataLoader(siamese_dataset_test,\n",
    "                        batch_size=16, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.backbone = models.__dict__[\"resnet18\"](progress=True, weights='DEFAULT')\n",
    "        out_features = list(self.backbone.modules())[-1].out_features\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "        nn.Linear(out_features , 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.backbone(x)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1,input2):\n",
    "        # output the representations of both images\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1,output2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=1.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        pos = (1-label) * torch.pow(euclidean_distance, 2)\n",
    "        neg = (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        loss_contrastive = torch.mean( pos + neg)\n",
    "        return loss_contrastive"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def evaluate_pair(output1,output2,target,threshold):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    cond = euclidean_distance<threshold\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0\n",
    "    pos_acc = 0\n",
    "    neg_acc = 0\n",
    "    # count accuracy of positive and negative pairs\n",
    "    for i in range(len(cond)):\n",
    "        if target[i]:\n",
    "            neg_sum+=1\n",
    "            if not cond[i]:\n",
    "                neg_acc+=1\n",
    "        if not target[i]:\n",
    "            pos_sum+=1\n",
    "            if cond[i]:\n",
    "                pos_acc+=1\n",
    "\n",
    "    return pos_acc,pos_sum,neg_acc,neg_sum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Initialize network\n",
    "model = SiameseNetwork()\n",
    "model = model.cuda()\n",
    "\n",
    "## Initialize optimizer\n",
    "optim = torch.optim.Adam(model.parameters(),lr=0.005)\n",
    "\n",
    "## Initialize loss\n",
    "criterion = ContrastiveLoss()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "for epoch in range(1000):\n",
    "    train_epoch_loss = 0\n",
    "    model.train()\n",
    "    c = 0\n",
    "    for i,(input1,input2,target) in enumerate(trainloader):\n",
    "        c+=1\n",
    "        optim.zero_grad()\n",
    "        output1,output2 = model(input1.cuda(),input2.cuda())\n",
    "        out = model(input1.cuda(),input2.cuda())\n",
    "\n",
    "        loss = criterion(output1,output2,target.cuda())\n",
    "        train_epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    train_epoch_loss /= c\n",
    "    train_loss.append(train_epoch_loss)\n",
    "\n",
    "    print(\"Epoch [{}/{}] ----> Training loss :{} \\n\".format(epoch+1,1000,train_epoch_loss))\n",
    "    # train loop ended\n",
    "    # start evaluation\n",
    "\n",
    "    valid_epoch_loss = 0\n",
    "    val_pos_accuracy = 0\n",
    "    val_neg_accuracy = 0\n",
    "    num_pos = 0\n",
    "    num_neg = 0\n",
    "\n",
    "    val_pos_accuracy2 = 0\n",
    "    val_neg_accuracy2= 0\n",
    "    num_pos2 = 0\n",
    "    num_neg2 = 0\n",
    "    model.eval()\n",
    "    c  =0\n",
    "    for i,(input1,input2,target) in enumerate(testloader):\n",
    "        c +=1\n",
    "        output1,output2 = model(input1.cuda(),input2.cuda())\n",
    "        loss = criterion(output1,output2,target.cuda())\n",
    "        valid_epoch_loss += loss.item()\n",
    "        pos_acc,pos_sum,neg_acc,neg_sum = evaluate_pair(output1,output2,target.cuda(),0.5)\n",
    "        pos_acc2,pos_sum2,neg_acc2,neg_sum2 = evaluate_pair(output1,output2,target.cuda(),0.35)\n",
    "        val_pos_accuracy+=pos_acc\n",
    "        val_neg_accuracy+=neg_acc\n",
    "        num_pos+=pos_sum\n",
    "        num_neg+=neg_sum\n",
    "        val_pos_accuracy2+=pos_acc2\n",
    "        val_neg_accuracy2+=neg_acc2\n",
    "        num_pos2+=pos_sum2\n",
    "        num_neg2+=neg_sum2\n",
    "\n",
    "    valid_epoch_loss /= c\n",
    "    val_pos_accuracy /= num_pos\n",
    "    val_neg_accuracy /= num_neg\n",
    "    val_pos_accuracy2 /= num_pos2\n",
    "    val_neg_accuracy2 /= num_neg2\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Validation loss :{} \\t\\t\\t P Acc : {}, N Acc: {} P Acc : {}, N Acc : {}\\n\".format(valid_epoch_loss,val_pos_accuracy,val_neg_accuracy, val_pos_accuracy2, val_neg_accuracy2))\n",
    "    # Save model\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"backbone\": \"resnet-18\",\n",
    "                \"optimizer_state_dict\": optim.state_dict()\n",
    "            },\n",
    "            os.path.join(\"siamese\", \"epoch_{}.pth\".format(epoch + 1))\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict\n",
    "img1 = r\"D:\\PycharmProjects\\AISS_Seminar\\yolov5\\pred_folder\\true\\vlcsnap-2023-06-27-23h11m53s160.png\"\n",
    "img2 = r\"D:\\PycharmProjects\\AISS_Seminar\\yolov5\\pred_folder\\generated\\5\\vlcsnap-2023-06-27-23h06m59s656_2.jpg\"\n",
    "transform = transforms.Compose([transforms.Resize((100, 100)),transforms.ToTensor()])\n",
    "img1 = transform(PIL.Image.open(img1)).cuda()\n",
    "img2 = transform(PIL.Image.open(img2)).cuda()\n",
    "img1 = img1[None, :]\n",
    "img2 = img2[None, :]\n",
    "output1, output2 = model(img1, img2)\n",
    "print(F.pairwise_distance(output1, output2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}