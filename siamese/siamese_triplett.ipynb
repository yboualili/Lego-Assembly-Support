{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self,imageFolderDataset,false_dataset, transform=None):\n",
    "        self.imageFolderDataset = imageFolderDataset\n",
    "        self.false_dataset = false_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "        folder = r\"D:\\PycharmProjects\\AISS\\siamese\\dataset_true\"\n",
    "        self.reference_imgs0 = self.prepare_imgs(os.path.join(folder,\"0\", \"82_14491.jpg\"), anchor=True)\n",
    "\n",
    "\n",
    "        self.reference_imgs1 = self.prepare_imgs(os.path.join(folder,\"1\", \"42_14314.jpg\"), anchor=True)\n",
    "\n",
    "\n",
    "        self.reference_imgs2 = self.prepare_imgs(os.path.join(folder,\"2\", \"02_3176.jpg\"), anchor=True)\n",
    "\n",
    "\n",
    "        self.reference_imgs3 = self.prepare_imgs(os.path.join(folder,\"3\", \"310_18578.jpg\"), anchor=True)\n",
    "\n",
    "    def prepare_imgs(self, p, anchor=False):\n",
    "        if anchor:\n",
    "            img = PIL.Image.open(p)\n",
    "        else:\n",
    "            img = p\n",
    "        width, height = img.size\n",
    "        if width > height:\n",
    "            img = img.transpose(Image.TRANSPOSE)\n",
    "        transform = transforms.Compose([transforms.Resize((150, 50)),transforms.ToTensor()])\n",
    "        img = transform(img)\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        # select a random stage\n",
    "        class_ = random.randint(0,3)\n",
    "\n",
    "        if class_ == 0:\n",
    "            img0 = self.reference_imgs0\n",
    "        if class_ == 1:\n",
    "            img0 = self.reference_imgs1\n",
    "\n",
    "        if class_ == 2:\n",
    "            img0 = self.reference_imgs2\n",
    "\n",
    "        if class_ == 3:\n",
    "            img0 = self.reference_imgs3\n",
    "\n",
    "\n",
    "\n",
    "        while True:\n",
    "            #Look untill the same stage image is found\n",
    "            img1_tuple = random.choice(self.imageFolderDataset)\n",
    "            if class_ == img1_tuple[1]:\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "            #Look untill the same stage image is found\n",
    "            img2_tuple = random.choice(self.false_dataset)\n",
    "            if class_ == img2_tuple[1]:\n",
    "                break\n",
    "\n",
    "\n",
    "        img1 = self.prepare_imgs(img1_tuple[0])\n",
    "        img2 = self.prepare_imgs(img2_tuple[0])\n",
    "\n",
    "        return img0,img1 ,img2\n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "folder_dataset = datasets.ImageFolder(root=r\"D:\\PycharmProjects\\AISS\\siamese\\dataset_true\")\n",
    "false_dataset = datasets.ImageFolder(root=r\"D:\\PycharmProjects\\AISS\\siamese\\dataset_false\")\n",
    "\n",
    "folder_dataset_test = datasets.ImageFolder(root=r\"D:\\PycharmProjects\\AISS\\siamese\\dataset_true_val\")\n",
    "false_dataset_test = datasets.ImageFolder(root=r\"D:\\PycharmProjects\\AISS\\siamese\\dataset_false_val\")\n",
    "\n",
    "# Resize the images and transform to tensors\n",
    "transformation = transforms.Compose([\n",
    "                                    transforms.Resize((150, 50)),\n",
    "                                     transforms.ToTensor()\n",
    "\n",
    "                                    ])\n",
    "\n",
    "transformation_test = transforms.Compose([\n",
    "                                    transforms.Resize((150, 50)),\n",
    "                                     transforms.ToTensor()\n",
    "                                    ])\n",
    "\n",
    "# Initialize the dataset\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,false_dataset=false_dataset,\n",
    "                                        transform=transformation)\n",
    "\n",
    "siamese_dataset_test = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,false_dataset=false_dataset_test,\n",
    "                                        transform=transformation_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# create the dataloader\n",
    "trainloader = DataLoader(siamese_dataset,\n",
    "                        batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "testloader = DataLoader(siamese_dataset_test,\n",
    "                        batch_size=32, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, backbone=\"resnet18\"):\n",
    "        '''\n",
    "        Creates a siamese network with a network from torchvision.models as backbone.\n",
    "            Parameters:\n",
    "                    backbone (str): Options of the backbone networks can be found at https://pytorch.org/vision/stable/models.html\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        # Create a backbone network from the pretrained models provided in torchvision.models\n",
    "        self.Feature_Extractor = resnet18(pretrained=True)\n",
    "        # freeze layers\n",
    "        for j, child in enumerate(self.Feature_Extractor.children()):\n",
    "            if j < 4:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "        num_filters = self.Feature_Extractor.fc.in_features\n",
    "\n",
    "        # feature representation head\n",
    "        self.Feature_Extractor.fc = nn.Sequential(\n",
    "                  nn.Linear(num_filters,512),\n",
    "                  nn.LeakyReLU(),\n",
    "                  nn.Linear(512,10))\n",
    "        self.Triplet_Loss = nn.Sequential(\n",
    "                  nn.Linear(10,2))\n",
    "    def forward(self,x):\n",
    "        features = self.Feature_Extractor(x)\n",
    "        triplets = self.Triplet_Loss(features)\n",
    "        return triplets\n",
    "\n",
    "    def forward(self, img1):\n",
    "        # Pass one image through the network and get the representation\n",
    "        feat1 = self.Feature_Extractor(img1)\n",
    "        output = self.Triplet_Loss(feat1)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    def calc_euclidean(self, x1, x2):\n",
    "        return (x1 - x2).pow(2).sum(1)\n",
    "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
    "        distance_positive = self.calc_euclidean(anchor, positive)\n",
    "        distance_negative = self.calc_euclidean(anchor, negative)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNetwork()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = TripletLoss()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(300):\n",
    "    print(\"[{} / {}]\".format(epoch, 300))\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    # Training Loop Start\n",
    "    model.train()\n",
    "    for img1, img2, img3 in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        img1, img2, img3 = map(lambda x: x.to(device), [img1, img2, img3])\n",
    "\n",
    "        anchor_out = model(img1)\n",
    "        positive_out = model(img2)\n",
    "        negative_out = model(img3)\n",
    "\n",
    "\n",
    "        loss = criterion(anchor_out, positive_out, negative_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "    print(\"\\tTraining: Loss={:.2f}\\t \".format(np.mean(losses)))\n",
    "    # Training Loop End\n",
    "    # start evaluation\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = []\n",
    "    for img1, img2, img3 in testloader:\n",
    "\n",
    "        img1, img2, img3 = map(lambda x: x.to(device), [img1, img2, img3])\n",
    "\n",
    "        anchor_out = model(img1)\n",
    "        positive_out = model(img2)\n",
    "        negative_out = model(img3)\n",
    "\n",
    "\n",
    "        loss = criterion(anchor_out, positive_out, negative_out)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "    l = np.mean(val_loss)\n",
    "    print(\"Validation loss :{}\".format(l))\n",
    "\n",
    "\n",
    "\n",
    "    # Save model\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"backbone\": \"resnet-18\",\n",
    "                \"optimizer_state_dict\": optimizer.state_dict()\n",
    "            },\n",
    "            os.path.join(\"siamese\",\"models_new\", \"epoch_{}_{}.pth\".format(epoch + 1, 2))\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load in model\n",
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load(\"siamese/models/epoch_35_0.8938460690362686.pth\")[\"model_state_dict\"])\n",
    "model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict with anchor image\n",
    "model.eval()\n",
    "\n",
    "img2 = PIL.Image.open(r\"D:\\PycharmProjects\\AISS\\siamese\\dataset_true\\1\\10_2830.jpg\")\n",
    "img1 = PIL.Image.open(r\"D:\\PycharmProjects\\AISS\\testbild.jpg\")\n",
    "#img1 = PIL.Image.open(r\"D:\\PycharmProjects\\AISS_Seminar\\yolov5\\pred_folder\\generated\\5\\vlcsnap-2023-06-28-20h53m36s380_1.jpg\")\n",
    "\n",
    "width, height = img1.size\n",
    "if width > height:\n",
    "    img1 = img1.transpose(Image.TRANSPOSE)\n",
    "width, height = img2.size\n",
    "if width > height:\n",
    "    img2 = img2.transpose(Image.TRANSPOSE)\n",
    "transform = transforms.Compose([transforms.Resize((150, 50)),transforms.ToTensor()])\n",
    "img1 = transform(img1).cuda()\n",
    "img2 = transform(img2).cuda()\n",
    "img1 = img1[None, :]\n",
    "img2 = img2[None, :]\n",
    "outputs = model(img1)\n",
    "print(outputs.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}